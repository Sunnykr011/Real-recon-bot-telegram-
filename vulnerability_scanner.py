"""
Advanced Vulnerability Scanner using Google Dorks
Implements smart scanning with rate limiting and result validation
"""

import requests
import time
import random
import logging
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
from google_dorks import GoogleDorksDatabase
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import trafilatura

logger = logging.getLogger(__name__)

class VulnerabilityScanner:
    def __init__(self):
        self.dorks_db = GoogleDorksDatabase()
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]
        self.session = requests.Session()
        self.rate_limit_delay = 2  # seconds between requests
        self.max_retries = 3
        
    def comprehensive_scan(self, domain):
        """Run comprehensive vulnerability scan with all dorks"""
        logger.info(f"Starting comprehensive scan for {domain}")
        
        all_dorks = self.dorks_db.get_all_dorks()
        results = {
            'domain': domain,
            'vulnerabilities': [],
            'total_dorks_tested': len(all_dorks),
            'successful_hits': 0,
            'categories_found': set()
        }
        
        # Use thread pool for concurrent scanning
        with ThreadPoolExecutor(max_workers=5) as executor:
            future_to_dork = {
                executor.submit(self._test_single_dork, domain, category, dork): (category, dork) 
                for category, dork in all_dorks
            }
            
            for future in as_completed(future_to_dork):
                category, dork = future_to_dork[future]
                try:
                    dork_result = future.result()
                    if dork_result:
                        results['vulnerabilities'].append(dork_result)
                        results['successful_hits'] += 1
                        results['categories_found'].add(category)
                        logger.info(f"Vulnerability found: {category} - {dork_result['title']}")
                except Exception as e:
                    logger.error(f"Error testing dork {dork}: {str(e)}")
        
        results['categories_found'] = list(results['categories_found'])
        logger.info(f"Comprehensive scan completed: {results['successful_hits']} vulnerabilities found")
        return results
    
    def dork_scan_only(self, domain):
        """Run Google dork scan only"""
        return self.comprehensive_scan(domain)
    
    def quick_scan(self, domain):
        """Run quick scan with high-priority dorks only"""
        logger.info(f"Starting quick scan for {domain}")
        
        high_priority_dorks = self.dorks_db.get_high_priority_dorks()
        results = {
            'domain': domain,
            'vulnerabilities': [],
            'total_dorks_tested': len(high_priority_dorks),
            'successful_hits': 0,
            'categories_found': set()
        }
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            future_to_dork = {
                executor.submit(self._test_single_dork, domain, category, dork): (category, dork) 
                for category, dork in high_priority_dorks
            }
            
            for future in as_completed(future_to_dork):
                category, dork = future_to_dork[future]
                try:
                    dork_result = future.result()
                    if dork_result:
                        results['vulnerabilities'].append(dork_result)
                        results['successful_hits'] += 1
                        results['categories_found'].add(category)
                except Exception as e:
                    logger.error(f"Error in quick scan dork {dork}: {str(e)}")
        
        results['categories_found'] = list(results['categories_found'])
        logger.info(f"Quick scan completed: {results['successful_hits']} vulnerabilities found")
        return results
    
    def _test_single_dork(self, domain, category, dork_template):
        """Test a single Google dork"""
        try:
            # Apply rate limiting
            time.sleep(random.uniform(1, self.rate_limit_delay))
            
            dork_query = dork_template.format(domain=domain)
            from urllib.parse import quote
            google_search_url = f"https://www.google.com/search?q={quote(dork_query)}"
            
            # Rotate user agent
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive'
            }
            
            for attempt in range(self.max_retries):
                try:
                    response = self.session.get(google_search_url, headers=headers, timeout=10)
                    
                    if response.status_code == 429:  # Rate limited
                        logger.warning("Rate limited by Google, increasing delay")
                        time.sleep(5 * (attempt + 1))
                        continue
                    
                    if response.status_code == 200:
                        return self._parse_google_results(domain, category, dork_query, response.text)
                    
                except requests.RequestException as e:
                    logger.warning(f"Request failed (attempt {attempt + 1}): {str(e)}")
                    if attempt < self.max_retries - 1:
                        time.sleep(2 ** attempt)
                    
            return None
            
        except Exception as e:
            logger.error(f"Error testing dork for {domain}: {str(e)}")
            return None
    
    def _parse_google_results(self, domain, category, dork_query, html_content):
        """Parse Google search results and validate findings"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Find search result links
        result_links = []
        for result in soup.find_all('div', class_='g'):
            link_elem = result.find('a')
            if link_elem and hasattr(link_elem, 'get') and link_elem.get('href'):
                url = str(link_elem.get('href'))
                if url.startswith('/url?q='):
                    # Extract actual URL from Google redirect
                    actual_url = url.split('/url?q=')[1].split('&')[0]
                    from urllib.parse import unquote
                    result_links.append(unquote(actual_url))
                elif url.startswith('http'):
                    result_links.append(url)
        
        # Filter results to only include target domain
        domain_results = [url for url in result_links if domain in url]
        
        if not domain_results:
            return None
        
        # Validate the first result
        validated_result = self._validate_vulnerability(domain_results[0], category)
        
        if validated_result:
            return {
                'category': category,
                'severity': self._calculate_severity(category),
                'title': self._generate_title(category),
                'description': self._generate_description(category),
                'url': validated_result['url'],
                'evidence': validated_result['evidence'],
                'dork_query': dork_query,
                'total_results': len(domain_results),
                'confidence': validated_result['confidence']
            }
        
        return None
    
    def _validate_vulnerability(self, url, category):
        """Validate vulnerability by analyzing the target URL"""
        try:
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
            }
            
            response = self.session.get(url, headers=headers, timeout=10)
            
            if response.status_code != 200:
                return None
            
            # Use trafilatura to extract clean text content
            text_content = trafilatura.extract(response.text)
            if not text_content:
                text_content = response.text
            
            # Category-specific validation
            validation_result = self._category_specific_validation(category, text_content, response.text)
            
            if validation_result['is_valid']:
                return {
                    'url': url,
                    'evidence': validation_result['evidence'],
                    'confidence': validation_result['confidence']
                }
            
            return None
            
        except Exception as e:
            logger.warning(f"Error validating URL {url}: {str(e)}")
            return None
    
    def _category_specific_validation(self, category, text_content, raw_html):
        """Perform category-specific vulnerability validation"""
        if category == "sql_injection":
            sql_error_patterns = [
                r"mysql_fetch_array\(\)",
                r"Warning: mysql_",
                r"ORA-\d+:",
                r"Microsoft OLE DB Provider",
                r"ODBC Microsoft Access Driver",
                r"PostgreSQL query failed",
                r"SQL syntax.*MySQL"
            ]
            
            for pattern in sql_error_patterns:
                if re.search(pattern, text_content, re.IGNORECASE):
                    return {
                        'is_valid': True,
                        'evidence': f"SQL error pattern detected: {pattern}",
                        'confidence': 0.85
                    }
        
        elif category == "xss_vulnerabilities":
            if any(param in text_content.lower() for param in ["<script>", "alert(", "javascript:", "onerror="]):
                return {
                    'is_valid': True,
                    'evidence': "XSS-related content detected in response",
                    'confidence': 0.75
                }
        
        elif category == "local_file_inclusion":
            lfi_indicators = ["root:x:0:0", "/etc/passwd", "boot.ini", "windows/system32"]
            if any(indicator in text_content.lower() for indicator in lfi_indicators):
                return {
                    'is_valid': True,
                    'evidence': "System file content detected",
                    'confidence': 0.90
                }
        
        elif category == "exposed_files":
            sensitive_patterns = [
                r"CREATE TABLE",
                r"INSERT INTO",
                r"password\s*=",
                r"DB_PASSWORD",
                r"database.*password"
            ]
            
            for pattern in sensitive_patterns:
                if re.search(pattern, text_content, re.IGNORECASE):
                    return {
                        'is_valid': True,
                        'evidence': f"Sensitive data pattern: {pattern}",
                        'confidence': 0.80
                    }
        
        elif category == "admin_panels":
            admin_indicators = ["admin", "login", "username", "password", "dashboard"]
            if sum(1 for indicator in admin_indicators if indicator in text_content.lower()) >= 3:
                return {
                    'is_valid': True,
                    'evidence': "Admin panel interface detected",
                    'confidence': 0.70
                }
        
        elif category == "directory_listing":
            if "Index of" in text_content and "Parent Directory" in text_content:
                return {
                    'is_valid': True,
                    'evidence': "Directory listing confirmed",
                    'confidence': 0.95
                }
        
        # Default validation for other categories
        return {
            'is_valid': True,
            'evidence': "Google dork match confirmed",
            'confidence': 0.60
        }
    
    def _calculate_severity(self, category):
        """Calculate vulnerability severity based on category"""
        severity_mapping = {
            "sql_injection": "Critical",
            "xss_vulnerabilities": "High",
            "local_file_inclusion": "Critical", 
            "exposed_files": "High",
            "admin_panels": "Medium",
            "directory_listing": "Medium",
            "database_errors": "Medium",
            "login_pages": "Low",
            "api_endpoints": "Medium",
            "sensitive_parameters": "Medium"
        }
        return severity_mapping.get(category, "Low")
    
    def _generate_title(self, category):
        """Generate vulnerability title"""
        titles = {
            "sql_injection": "SQL Injection Vulnerability Detected",
            "xss_vulnerabilities": "Cross-Site Scripting (XSS) Vulnerability",
            "local_file_inclusion": "Local File Inclusion (LFI) Vulnerability",
            "exposed_files": "Sensitive Files Exposure",
            "admin_panels": "Administrative Panel Accessible",
            "directory_listing": "Directory Listing Enabled",
            "database_errors": "Database Error Information Disclosure",
            "login_pages": "Login Page Discovered",
            "api_endpoints": "API Endpoint Exposed",
            "sensitive_parameters": "Sensitive Parameters in URLs"
        }
        return titles.get(category, "Potential Security Issue")
    
    def _generate_description(self, category):
        """Generate vulnerability description"""
        descriptions = {
            "sql_injection": "SQL injection vulnerability allows attackers to manipulate database queries, potentially leading to data breaches, unauthorized access, and system compromise.",
            "xss_vulnerabilities": "Cross-site scripting vulnerability allows attackers to inject malicious scripts into web pages, potentially stealing user sessions and sensitive data.",
            "local_file_inclusion": "Local file inclusion vulnerability allows attackers to include local files on the server, potentially exposing sensitive system files and configuration data.",
            "exposed_files": "Sensitive files are publicly accessible, potentially exposing database credentials, configuration details, and other confidential information.",
            "admin_panels": "Administrative interfaces are accessible without proper access controls, potentially allowing unauthorized system administration.",
            "directory_listing": "Directory listing is enabled, revealing server directory structure and potentially sensitive files to unauthorized users.",
            "database_errors": "Database errors are exposed to users, revealing internal system information that could aid in further attacks.",
            "login_pages": "Login interfaces have been discovered, which could be targets for brute force attacks and credential stuffing.",
            "api_endpoints": "API endpoints are exposed, potentially revealing application functionality and data access points.",
            "sensitive_parameters": "URLs contain sensitive parameters that may expose debugging information or internal application states."
        }
        return descriptions.get(category, "Potential security vulnerability identified through Google dorking.")
